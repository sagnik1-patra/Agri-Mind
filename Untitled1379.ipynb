{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38cc50f3-065c-4fde-9282-bdb47e6be107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading CSVs...\n",
      "[INFO] Using pre-merged dataset: C:\\Users\\sagni\\Downloads\\Agri Mind\\archive\\yield_df.csv (shape=(28242, 8))\n",
      "[INFO] Dropping likely row-id columns: ['Unnamed: 0']\n",
      "[INFO] Target column detected: hg/ha_yield\n",
      "[INFO] Fitting preprocessing pipeline...\n",
      "[INFO] Building Keras model with input_dim=115 ...\n",
      "[INFO] Training...\n",
      "Epoch 1/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 12881128448.0000 - mae: 76141.7031 - val_loss: 13188225024.0000 - val_mae: 77052.6875 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 12832051200.0000 - mae: 76007.4531 - val_loss: 13164433408.0000 - val_mae: 76995.9297 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 12685656064.0000 - mae: 75646.3281 - val_loss: 13134403584.0000 - val_mae: 76925.4062 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 12380301312.0000 - mae: 74895.5547 - val_loss: 13000432640.0000 - val_mae: 76594.6719 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 11856868352.0000 - mae: 73579.4297 - val_loss: 12415139840.0000 - val_mae: 75171.2891 - learning_rate: 0.0010\n",
      "Epoch 6/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 11086903296.0000 - mae: 71573.6094 - val_loss: 11345718272.0000 - val_mae: 72402.7812 - learning_rate: 0.0010\n",
      "Epoch 7/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 10070978560.0000 - mae: 68775.1953 - val_loss: 9960465408.0000 - val_mae: 68625.7500 - learning_rate: 0.0010\n",
      "Epoch 8/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 8796338176.0000 - mae: 64834.3242 - val_loss: 8117790720.0000 - val_mae: 62898.4258 - learning_rate: 0.0010\n",
      "Epoch 9/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 7346403840.0000 - mae: 59941.9727 - val_loss: 6104830464.0000 - val_mae: 55121.8047 - learning_rate: 0.0010\n",
      "Epoch 10/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 5825965568.0000 - mae: 53951.4844 - val_loss: 4454454784.0000 - val_mae: 47373.2969 - learning_rate: 0.0010\n",
      "Epoch 11/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 4362114560.0000 - mae: 46981.6680 - val_loss: 3054608640.0000 - val_mae: 39204.1758 - learning_rate: 0.0010\n",
      "Epoch 12/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3087867648.0000 - mae: 39548.7852 - val_loss: 1914112640.0000 - val_mae: 31298.2207 - learning_rate: 0.0010\n",
      "Epoch 13/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2053451904.0000 - mae: 32274.6191 - val_loss: 1235897216.0000 - val_mae: 25287.7402 - learning_rate: 0.0010\n",
      "Epoch 14/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1319524992.0000 - mae: 25715.9238 - val_loss: 687086528.0000 - val_mae: 18409.7188 - learning_rate: 0.0010\n",
      "Epoch 15/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 849230784.0000 - mae: 20488.0176 - val_loss: 429913920.0000 - val_mae: 14006.3086 - learning_rate: 0.0010\n",
      "Epoch 16/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 575744640.0000 - mae: 16720.0371 - val_loss: 279551264.0000 - val_mae: 10830.9482 - learning_rate: 0.0010\n",
      "Epoch 17/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 433369024.0000 - mae: 14490.2715 - val_loss: 243750944.0000 - val_mae: 10136.8008 - learning_rate: 0.0010\n",
      "Epoch 18/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 376154336.0000 - mae: 13401.4414 - val_loss: 235974688.0000 - val_mae: 10269.8379 - learning_rate: 0.0010\n",
      "Epoch 19/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 354428160.0000 - mae: 12992.6650 - val_loss: 217983344.0000 - val_mae: 9821.5518 - learning_rate: 0.0010\n",
      "Epoch 20/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 336220064.0000 - mae: 12588.4043 - val_loss: 215483744.0000 - val_mae: 9877.6836 - learning_rate: 0.0010\n",
      "Epoch 21/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 330718656.0000 - mae: 12415.2920 - val_loss: 201854368.0000 - val_mae: 9379.6309 - learning_rate: 0.0010\n",
      "Epoch 22/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 319597696.0000 - mae: 12207.1074 - val_loss: 194707424.0000 - val_mae: 9397.2686 - learning_rate: 0.0010\n",
      "Epoch 23/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 310937632.0000 - mae: 11955.4453 - val_loss: 193222464.0000 - val_mae: 9242.2139 - learning_rate: 0.0010\n",
      "Epoch 24/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 303437312.0000 - mae: 11798.0684 - val_loss: 175559904.0000 - val_mae: 8506.1582 - learning_rate: 0.0010\n",
      "Epoch 25/25\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 304890208.0000 - mae: 11793.3496 - val_loss: 162563136.0000 - val_mae: 7767.9888 - learning_rate: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Metrics:\n",
      "{\n",
      "  \"train\": {\n",
      "    \"mae\": 7155.47090473934,\n",
      "    \"mse\": 135786991.2866709,\n",
      "    \"rmse\": 11652.767537656919,\n",
      "    \"r2\": 0.9811623953156794\n",
      "  },\n",
      "  \"test\": {\n",
      "    \"mae\": 7767.988619101578,\n",
      "    \"mse\": 162563116.4044613,\n",
      "    \"rmse\": 12750.024172701058,\n",
      "    \"r2\": 0.977588851955616\n",
      "  },\n",
      "  \"sklearn_version\": \"1.7.1\",\n",
      "  \"tensorflow_version\": \"2.18.0\",\n",
      "  \"rows\": 28242,\n",
      "  \"train_rows\": 22593,\n",
      "  \"test_rows\": 5649,\n",
      "  \"target\": \"hg/ha_yield\"\n",
      "}\n",
      "[INFO] Saved:\n",
      "   C:\\Users\\sagni\\Downloads\\Agri Mind\\neuro_preprocess.pkl\n",
      "   C:\\Users\\sagni\\Downloads\\Agri Mind\\neuro_model.h5\n",
      "   C:\\Users\\sagni\\Downloads\\Agri Mind\\neuro_metrics.json\n",
      "   C:\\Users\\sagni\\Downloads\\Agri Mind\\neuro_config.yaml\n",
      "   C:\\Users\\sagni\\Downloads\\Agri Mind\\neuro_feature_names.json\n",
      "   C:\\Users\\sagni\\Downloads\\Agri Mind\\neuro_history.csv\n",
      "[INFO] DONE.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "AgriMind training script (yield regression) with robust metrics (RMSE-safe)\n",
    "and sklearn compatibility fixes.\n",
    "\n",
    "Artifacts saved to BASE_DIR:\n",
    "- neuro_preprocess.pkl\n",
    "- neuro_model.h5\n",
    "- neuro_metrics.json\n",
    "- neuro_config.yaml\n",
    "- neuro_feature_names.json\n",
    "- neuro_history.csv\n",
    "\"\"\"\n",
    "import os, json, math, csv, sys, time, random, pathlib, warnings\n",
    "from typing import Tuple, List, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Paths (edit if needed) ----\n",
    "BASE_DIR = r\"C:\\Users\\sagni\\Downloads\\Agri Mind\"\n",
    "ARCHIVE  = os.path.join(BASE_DIR, \"archive\")\n",
    "\n",
    "PTH_PEST = os.path.join(ARCHIVE, \"pesticides.csv\")\n",
    "PTH_RAIN = os.path.join(ARCHIVE, \"rainfall.csv\")\n",
    "PTH_TEMP = os.path.join(ARCHIVE, \"temp.csv\")\n",
    "PTH_YLD  = os.path.join(ARCHIVE, \"yield.csv\")\n",
    "PTH_PREMERGED = os.path.join(ARCHIVE, \"yield_df.csv\")  # if present, uses this\n",
    "\n",
    "# ---- Artifact filenames ----\n",
    "PKL_OUT   = os.path.join(BASE_DIR, \"neuro_preprocess.pkl\")\n",
    "H5_OUT    = os.path.join(BASE_DIR, \"neuro_model.h5\")\n",
    "METRICSJS = os.path.join(BASE_DIR, \"neuro_metrics.json\")\n",
    "CONFYAML  = os.path.join(BASE_DIR, \"neuro_config.yaml\")\n",
    "FEATJSON  = os.path.join(BASE_DIR, \"neuro_feature_names.json\")\n",
    "HISTCSV   = os.path.join(BASE_DIR, \"neuro_history.csv\")\n",
    "\n",
    "# ---- ML stack ----\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import joblib\n",
    "import yaml\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "pd.options.display.width = 180\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def rmse_safe(y_true, y_pred) -> float:\n",
    "    \"\"\"RMSE that works with old/new sklearn versions.\"\"\"\n",
    "    try:\n",
    "        # sklearn >= 0.22\n",
    "        return float(mean_squared_error(y_true, y_pred, squared=False))\n",
    "    except TypeError:\n",
    "        # very old sklearn\n",
    "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def to_numpy(X):\n",
    "    \"\"\"Convert possibly-sparse matrix to a dense numpy array for Keras.\"\"\"\n",
    "    if hasattr(X, \"toarray\"):\n",
    "        return X.toarray()\n",
    "    return np.asarray(X)\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed); tf.random.set_seed(seed)\n",
    "\n",
    "def print_info(msg: str):\n",
    "    print(f\"[INFO] {msg}\")\n",
    "\n",
    "def guess_key_cols(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Drop obvious row/id columns present in some Kaggle CSVs.\"\"\"\n",
    "    cands = []\n",
    "    for c in df.columns:\n",
    "        low = c.lower()\n",
    "        if low in {\"unnamed: 0\", \"index\", \"id\"} or low.startswith(\"unnamed:\"):\n",
    "            cands.append(c)\n",
    "    return cands\n",
    "\n",
    "def load_data() -> Tuple[pd.DataFrame, str]:\n",
    "    \"\"\"Load and return dataframe + target column name.\"\"\"\n",
    "    print_info(\"Loading CSVs...\")\n",
    "    if os.path.exists(PTH_PREMERGED):\n",
    "        df = pd.read_csv(PTH_PREMERGED)\n",
    "        print_info(f\"Using pre-merged dataset: {PTH_PREMERGED} (shape={df.shape})\")\n",
    "    else:\n",
    "        # Minimal defensive merge (country / item / year)\n",
    "        if not all(os.path.exists(p) for p in [PTH_PEST, PTH_RAIN, PTH_TEMP, PTH_YLD]):\n",
    "            raise FileNotFoundError(\"Some required CSVs are missing in the archive folder.\")\n",
    "        pest = pd.read_csv(PTH_PEST)\n",
    "        rain = pd.read_csv(PTH_RAIN)\n",
    "        temp = pd.read_csv(PTH_TEMP)\n",
    "        yld  = pd.read_csv(PTH_YLD)\n",
    "\n",
    "        # Standardize keys (best-effort—adapt if your headers differ)\n",
    "        for d in (pest, rain, temp, yld):\n",
    "            # normalize column names\n",
    "            d.columns = [c.strip() for c in d.columns]\n",
    "\n",
    "        # pick likely keys\n",
    "        key_cols = []\n",
    "        for k in [\"Area\", \"area\", \"Country\", \"country\"]:\n",
    "            if k in yld.columns: key_cols.append(k)\n",
    "        for k in [\"Item\", \"Crop\", \"item\"]:\n",
    "            if k in yld.columns and k not in key_cols: key_cols.append(k)\n",
    "        for k in [\"Year\", \"year\"]:\n",
    "            if k in yld.columns and k not in key_cols: key_cols.append(k)\n",
    "\n",
    "        if not key_cols:\n",
    "            raise ValueError(\"Could not infer merge keys; please ensure shared keys (Area/Item/Year).\")\n",
    "\n",
    "        df = yld.copy()\n",
    "        for m, name in [(pest, \"pest\"), (rain, \"rain\"), (temp, \"temp\")]:\n",
    "            join_keys = [k for k in key_cols if k in m.columns]\n",
    "            df = df.merge(m, on=join_keys, how=\"left\", suffixes=(\"\", f\"_{name}\"))\n",
    "\n",
    "        print_info(f\"Merged dataset shape = {df.shape}\")\n",
    "\n",
    "    # Drop obvious index columns\n",
    "    dropc = guess_key_cols(df)\n",
    "    if dropc:\n",
    "        print_info(f\"Dropping likely row-id columns: {dropc}\")\n",
    "        df = df.drop(columns=dropc)\n",
    "\n",
    "    # Detect target\n",
    "    possible_targets = [\"hg/ha_yield\", \"yield\", \"Yield\", \"target\", \"y\"]\n",
    "    target_col = None\n",
    "    for c in possible_targets:\n",
    "        if c in df.columns:\n",
    "            target_col = c\n",
    "            break\n",
    "    if target_col is None:\n",
    "        # default to last numeric column if nothing found (not ideal, but useful fallback)\n",
    "        num_only = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if not num_only:\n",
    "            raise ValueError(\"No numeric target found and no numeric columns to choose from.\")\n",
    "        target_col = num_only[-1]\n",
    "    print_info(f\"Target column detected: {target_col}\")\n",
    "    # Remove NA target rows\n",
    "    df = df[~df[target_col].isna()].copy()\n",
    "\n",
    "    return df, target_col\n",
    "\n",
    "def split_features(df: pd.DataFrame, target_col: str) -> Tuple[List[str], List[str]]:\n",
    "    X = df.drop(columns=[target_col])\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def build_preprocessor(df: pd.DataFrame, target_col: str) -> Tuple[Pipeline, List[str], List[str]]:\n",
    "    X = df.drop(columns=[target_col])\n",
    "    num_cols, cat_cols = split_features(df, target_col)\n",
    "\n",
    "    # scikit-learn >=1.2 uses 'sparse_output'; older uses 'sparse'\n",
    "    ohe_kwargs = dict(handle_unknown=\"ignore\")\n",
    "    try:\n",
    "        OneHotEncoder(sparse_output=True, **ohe_kwargs)\n",
    "        ohe = OneHotEncoder(sparse_output=True, **ohe_kwargs)\n",
    "    except TypeError:\n",
    "        # very old sklearn\n",
    "        ohe = OneHotEncoder(sparse=True, **ohe_kwargs)\n",
    "\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", ohe)\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3,\n",
    "    )\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", pre)])\n",
    "    return pipe, num_cols, cat_cols\n",
    "\n",
    "def get_feature_names(pre: ColumnTransformer, num_cols: List[str], cat_cols: List[str]) -> List[str]:\n",
    "    names = []\n",
    "    # numeric features pass-through after scaler\n",
    "    names.extend(num_cols)\n",
    "    # get OHE names\n",
    "    try:\n",
    "        ohe = pre.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "        ohe_names = ohe.get_feature_names_out(cat_cols).tolist()\n",
    "    except Exception:\n",
    "        # fallback if not available\n",
    "        ohe_names = [f\"{c}_ohe\" for c in cat_cols]\n",
    "    names.extend(ohe_names)\n",
    "    return names\n",
    "\n",
    "def build_model(input_dim: int) -> keras.Model:\n",
    "    inp = keras.Input(shape=(input_dim,), name=\"features\")\n",
    "    x = layers.Dense(256, activation=\"relu\")(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    out = layers.Dense(1, activation=\"linear\", name=\"yield\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inp, outputs=out, name=\"AgriMindYieldNet\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"mse\",\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    seed_everything(42)\n",
    "    os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "    # 1) Load data\n",
    "    df, target_col = load_data()\n",
    "\n",
    "    # 2) Build preprocessor & split\n",
    "    print_info(\"Fitting preprocessing pipeline...\")\n",
    "    pre_pipe, num_cols, cat_cols = build_preprocessor(df, target_col)\n",
    "\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col].astype(float)\n",
    "\n",
    "    Xtr_raw, Xte_raw, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # fit only on train\n",
    "    pre_pipe.fit(Xtr_raw)\n",
    "    Xtr = pre_pipe.transform(Xtr_raw)\n",
    "    Xte = pre_pipe.transform(Xte_raw)\n",
    "\n",
    "    input_dim = (Xtr.shape[1] if not hasattr(Xtr, \"toarray\") else Xtr.shape[1])\n",
    "    print_info(f\"Building Keras model with input_dim={input_dim} ...\")\n",
    "    model = build_model(input_dim=input_dim)\n",
    "\n",
    "    # 3) Train\n",
    "    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True)\n",
    "    rlrop = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-6)\n",
    "    csvlog = callbacks.CSVLogger(HISTCSV, append=False)\n",
    "\n",
    "    print_info(\"Training...\")\n",
    "    hist = model.fit(\n",
    "        to_numpy(Xtr), y_train.values,\n",
    "        validation_data=(to_numpy(Xte), y_test.values),\n",
    "        epochs=25,\n",
    "        batch_size=256,\n",
    "        verbose=1,\n",
    "        callbacks=[es, rlrop, csvlog],\n",
    "    )\n",
    "\n",
    "    # 4) Evaluate (with RMSE-safe)\n",
    "    y_pred_tr = model.predict(to_numpy(Xtr), verbose=0).ravel()\n",
    "    y_pred_te = model.predict(to_numpy(Xte), verbose=0).ravel()\n",
    "\n",
    "    metrics = {\n",
    "        \"train\": {\n",
    "            \"mae\": float(mean_absolute_error(y_train, y_pred_tr)),\n",
    "            \"mse\": float(mean_squared_error(y_train, y_pred_tr)),\n",
    "            \"rmse\": rmse_safe(y_train, y_pred_tr),\n",
    "            \"r2\":  float(r2_score(y_train, y_pred_tr)),\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"mae\": float(mean_absolute_error(y_test, y_pred_te)),\n",
    "            \"mse\": float(mean_squared_error(y_test, y_pred_te)),\n",
    "            \"rmse\": rmse_safe(y_test, y_pred_te),\n",
    "            \"r2\":  float(r2_score(y_test, y_pred_te)),\n",
    "        },\n",
    "        \"sklearn_version\": sklearn.__version__,\n",
    "        \"tensorflow_version\": tf.__version__,\n",
    "        \"rows\": int(len(df)),\n",
    "        \"train_rows\": int(len(Xtr_raw)),\n",
    "        \"test_rows\": int(len(Xte_raw)),\n",
    "        \"target\": target_col,\n",
    "    }\n",
    "    print_info(\"Metrics:\\n\" + json.dumps(metrics, indent=2))\n",
    "\n",
    "    # 5) Persist artifacts\n",
    "    # 5a) Feature names after fit\n",
    "    feat_names = get_feature_names(pre_pipe.named_steps[\"preprocess\"], num_cols, cat_cols)\n",
    "    with open(FEATJSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"feature_names\": feat_names}, f, indent=2)\n",
    "\n",
    "    # 5b) Save preprocess bundle\n",
    "    bundle = {\n",
    "        \"preprocess\": pre_pipe,\n",
    "        \"target_col\": target_col,\n",
    "        \"numeric_cols\": num_cols,\n",
    "        \"cat_cols\": cat_cols,\n",
    "        \"feature_names\": feat_names,\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"sklearn_version\": sklearn.__version__,\n",
    "    }\n",
    "    joblib.dump(bundle, PKL_OUT)\n",
    "\n",
    "    # 5c) Save model\n",
    "    model.save(H5_OUT)\n",
    "\n",
    "    # 5d) Save metrics\n",
    "    with open(METRICSJS, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # 5e) Save lightweight YAML config\n",
    "    cfg = {\n",
    "        \"paths\": {\n",
    "            \"base_dir\": BASE_DIR,\n",
    "            \"archive\": ARCHIVE,\n",
    "            \"preprocess_pkl\": PKL_OUT,\n",
    "            \"model_h5\": H5_OUT,\n",
    "            \"metrics_json\": METRICSJS,\n",
    "            \"feature_names_json\": FEATJSON,\n",
    "            \"history_csv\": HISTCSV,\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"random_seed\": 42,\n",
    "            \"test_size\": 0.2,\n",
    "            \"batch_size\": 256,\n",
    "            \"epochs\": 25,\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"loss\": \"mse\",\n",
    "            \"callbacks\": [\"EarlyStopping(patience=8)\", \"ReduceLROnPlateau(patience=4)\", \"CSVLogger\"],\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"AgriMindYieldNet\",\n",
    "            \"layers\": [\"Dense(256,relu)\", \"BN\", \"Dropout(0.25)\", \"Dense(128,relu)\", \"BN\", \"Dropout(0.25)\", \"Dense(64,relu)\", \"Dense(1,linear)\"],\n",
    "            \"input_dim\": int(input_dim),\n",
    "        },\n",
    "        \"target\": target_col,\n",
    "        \"columns\": {\n",
    "            \"numeric\": num_cols,\n",
    "            \"categorical\": cat_cols\n",
    "        },\n",
    "    }\n",
    "    with open(CONFYAML, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(cfg, f, sort_keys=False)\n",
    "\n",
    "    print_info(\"Saved:\")\n",
    "    for p in [PKL_OUT, H5_OUT, METRICSJS, CONFYAML, FEATJSON, HISTCSV]:\n",
    "        print(\"  \", p)\n",
    "    print_info(\"DONE.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c0734-9a3e-4a75-9275-5432ccd5b09a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
