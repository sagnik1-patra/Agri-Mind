{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1a006e-56aa-420d-b38a-9aea1743f66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metrics\": {\n",
      "    \"MAE\": 7277.987452027177,\n",
      "    \"MSE\": 141142785.69759548,\n",
      "    \"RMSE\": 11880.35292815813,\n",
      "    \"R2\": 0.9804440155211175\n",
      "  },\n",
      "  \"rows_scored\": 28242\n",
      "}\n",
      "[OK] Wrote predictions to: C:\\Users\\sagni\\Downloads\\Agri Mind\\predictions.csv\n",
      "   Area        Item  Year  actual   prediction\n",
      "Albania       Maize  1990   36613 20763.816406\n",
      "Albania    Potatoes  1990   66667 77022.804688\n",
      "Albania Rice, paddy  1990   23333 29063.724609\n",
      "Albania     Sorghum  1990   12500 13657.093750\n",
      "Albania    Soybeans  1990    7000  9684.549805\n",
      "Albania       Wheat  1990   30197 18972.654297\n",
      "Albania       Maize  1991   29068 22150.837891\n",
      "Albania    Potatoes  1991   77818 80981.031250\n",
      "Albania Rice, paddy  1991   28538 31464.589844\n",
      "Albania     Sorghum  1991    6667 16279.347656\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# USER PATHS (EDIT THESE)\n",
    "# ----------------------------\n",
    "BASE_DIR   = r\"C:\\Users\\sagni\\Downloads\\Agri Mind\"\n",
    "PKL_PATH   = os.path.join(BASE_DIR, \"neuro_preprocess.pkl\")  # your preprocess bundle\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"neuro_model.h5\")        # your trained model\n",
    "INPUT_CSV  = os.path.join(BASE_DIR, \"archive\", \"yield_df.csv\")  # CSV to score\n",
    "OUT_CSV    = os.path.join(BASE_DIR, \"predictions.csv\")          # where to save predictions\n",
    "\n",
    "# If your bundle does not have 'target_col', we try these fallbacks:\n",
    "POSSIBLE_TARGETS = [\"hg/ha_yield\", \"yield\", \"Yield\", \"target\", \"y\"]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _ensure_dir_for_file(path: str) -> None:\n",
    "    \"\"\"Create parent directory for a file if it doesn't exist.\"\"\"\n",
    "    d = os.path.dirname(path)\n",
    "    if d:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def _to_numpy(X):\n",
    "    \"\"\"Convert dense/sparse to numpy array.\"\"\"\n",
    "    return X.toarray() if hasattr(X, \"toarray\") else np.asarray(X)\n",
    "\n",
    "def _detect_target(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in POSSIBLE_TARGETS:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _align_to_preprocess_columns(df: pd.DataFrame, preprocess) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Many sklearn pipelines (esp. ColumnTransformer) expect the same columns seen at fit time.\n",
    "    We reindex the incoming DataFrame to match preprocess.feature_names_in_ when available,\n",
    "    adding any missing columns as NaN and dropping extras.\n",
    "    \"\"\"\n",
    "    cols_fit = getattr(preprocess, \"feature_names_in_\", None)\n",
    "    if cols_fit is None:\n",
    "        # Try to reach into the first step if it's a Pipeline with a ColumnTransformer\n",
    "        # Otherwise, just return df as-is.\n",
    "        return df\n",
    "\n",
    "    # Create any missing cols as NaN\n",
    "    aligned = df.reindex(columns=list(cols_fit))\n",
    "    return aligned\n",
    "\n",
    "def _load_bundle_and_model(pkl_path: str, model_path: str) -> Dict[str, Any]:\n",
    "    if not os.path.exists(pkl_path):\n",
    "        raise FileNotFoundError(f\"Preprocess bundle not found: {pkl_path}\")\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "    bundle = joblib.load(pkl_path)\n",
    "    if not isinstance(bundle, dict) or \"preprocess\" not in bundle:\n",
    "        raise ValueError(\"Invalid preprocess bundle: expected dict with key 'preprocess'.\")\n",
    "\n",
    "    preprocess = bundle[\"preprocess\"]\n",
    "    target_col = bundle.get(\"target_col\")  # may be None\n",
    "\n",
    "    # IMPORTANT: load without compiling to avoid legacy metric ('mse') issues\n",
    "    model = keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "    return {\"preprocess\": preprocess, \"target_col\": target_col, \"model\": model}\n",
    "\n",
    "def predict_file(\n",
    "    input_csv: str,\n",
    "    pkl_path: str,\n",
    "    model_path: str,\n",
    "    out_csv: str,\n",
    "    print_out: bool = True,\n",
    "    id_cols: Optional[List[str]] = None,\n",
    "    target_col: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load preprocess + model, score the CSV, and write predictions to out_csv.\n",
    "    Returns the output DataFrame.\n",
    "    \"\"\"\n",
    "    # 1) Load\n",
    "    info = _load_bundle_and_model(pkl_path, model_path)\n",
    "    preprocess = info[\"preprocess\"]\n",
    "    model = info[\"model\"]\n",
    "    bundle_target = info.get(\"target_col\")\n",
    "\n",
    "    # 2) Read input\n",
    "    if not os.path.exists(input_csv):\n",
    "        raise FileNotFoundError(f\"Input CSV not found: {input_csv}\")\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # 3) Determine target (if present; used only for metrics/cleanup)\n",
    "    tgt = target_col or bundle_target or _detect_target(df)\n",
    "\n",
    "    # 4) Optionally keep ID cols to pass through in the output\n",
    "    id_cols = id_cols or [c for c in [\"Area\", \"Item\", \"Year\", \"Country\", \"State\", \"id\", \"ID\"] if c in df.columns]\n",
    "    keep_cols = [c for c in id_cols if c in df.columns]\n",
    "\n",
    "    # 5) If target is present, drop it before transform (but keep for metrics)\n",
    "    y_true = None\n",
    "    if tgt and tgt in df.columns:\n",
    "        y_true = df[tgt].astype(float).values\n",
    "        X_df = df.drop(columns=[tgt])\n",
    "    else:\n",
    "        X_df = df\n",
    "\n",
    "    # 6) Align columns to what the pipeline expects\n",
    "    X_df = _align_to_preprocess_columns(X_df, preprocess)\n",
    "\n",
    "    # 7) Transform & predict\n",
    "    X_proc = preprocess.transform(X_df)\n",
    "    y_pred = model.predict(_to_numpy(X_proc), verbose=0).ravel()\n",
    "\n",
    "    # 8) Build output frame\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    for c in keep_cols:\n",
    "        out[c] = df[c]\n",
    "    if tgt and tgt in df.columns:\n",
    "        out[\"actual\"] = df[tgt].values\n",
    "    out[\"prediction\"] = y_pred\n",
    "\n",
    "    # 9) Save\n",
    "    _ensure_dir_for_file(out_csv)\n",
    "    out.to_csv(out_csv, index=False)\n",
    "\n",
    "    # 10) Quick metrics if actuals available\n",
    "    if y_true is not None:\n",
    "        mae = float(mean_absolute_error(y_true, y_pred))\n",
    "        mse = float(mean_squared_error(y_true, y_pred))\n",
    "        rmse = float(math.sqrt(mse))   # avoid sklearn `squared=False` arg for compatibility\n",
    "        r2 = float(r2_score(y_true, y_pred))\n",
    "        metrics = {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}\n",
    "        if print_out:\n",
    "            print(json.dumps({\"metrics\": metrics, \"rows_scored\": int(len(out))}, indent=2))\n",
    "    else:\n",
    "        if print_out:\n",
    "            print(json.dumps({\"metrics\": None, \"rows_scored\": int(len(out))}, indent=2))\n",
    "\n",
    "    if print_out:\n",
    "        print(f\"[OK] Wrote predictions to: {out_csv}\")\n",
    "        print(out.head(10).to_string(index=False))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Run directly (Option 2 style)\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    _ = predict_file(\n",
    "        input_csv=INPUT_CSV,\n",
    "        pkl_path=PKL_PATH,\n",
    "        model_path=MODEL_PATH,\n",
    "        out_csv=OUT_CSV,\n",
    "        print_out=True,\n",
    "        id_cols=None,       # or like [\"Area\",\"Item\",\"Year\"]\n",
    "        target_col=None,    # set if you want to override the bundle/auto-detected target\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4477e66-0c4a-44d6-8444-391b49bbf325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
